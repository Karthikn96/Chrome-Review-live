# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yuIUxr_mCaiw9KfWiYexZJnP5nciXTSl
"""

import streamlit as st
import streamlit.components.v1 as stc
import pandas as pd
import numpy as np
import string
import nltk
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.tokenize import WhitespaceTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import wordnet

st.title("Question 1 Part 2")
data_file = st.file_uploader("Upload CSV",type=["csv"])
st.header("To View the uploaded file, please click the below button")
if st.button("Display Data"):
    if data_file is not None:
        st.write(type(data_file))
        df = pd.read_csv(data_file)
        st.dataframe(df)
st.header("To find out the good user reviews with bad rating, please click the below button")
if st.button("Process"):
    if data_file is not None:
        df = pd.read_csv(data_file)
        appreviews = df
        appreviews.Text = appreviews.Text.astype(str)
        pd.set_option('max_rows', 99999)
        pd.set_option('max_colwidth', 60)
        appreviews = appreviews[["ID", "Text", "Star"]]
        appreviews['Text'] = appreviews.Text.str.replace("[^a-z'A-Z'0-9']", ' ')

        def get_wordnet_pos(pos_tag):
            if pos_tag.startswith('J'):
                return wordnet.ADJ
            elif pos_tag.startswith('V'):
                return wordnet.VERB
            elif pos_tag.startswith('N'):
                return wordnet.NOUN
            elif pos_tag.startswith('R'):
                return wordnet.ADV
            else:
                return wordnet.NOUN

        def clean_text(text):
            # lower text
            text = text.lower()
            # tokenize text and remove puncutation
            text = [word.strip(string.punctuation) for word in text.split(" ")]
            # remove stop words
            stop = stopwords.words('english')
            text = [x for x in text if x not in stop]
            # remove empty tokens
            text = [t for t in text if len(t) > 0]
            # pos tag text
            pos_tags = pos_tag(text)
            # lemmatize text
            text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]
            # remove words with only one letter
            text = [t for t in text if len(t) > 1]
            # join all
            text = " ".join(text)
            return (text)


        appreviews["review_clean"] = appreviews["Text"].apply(lambda x: clean_text(x))

        sid = SentimentIntensityAnalyzer()
        appreviews["sentiments"] = appreviews["Text"].apply(lambda x: sid.polarity_scores(x))
        appreviews = pd.concat([appreviews.drop(['sentiments'], axis=1), appreviews['sentiments'].apply(pd.Series)],
                               axis=1)
        # add number of characters column
        appreviews["nb_chars"] = appreviews["Text"].apply(lambda x: len(x))
        # add number of words column
        appreviews["nb_words"] = appreviews["Text"].apply(lambda x: len(x.split(" ")))
        # create doc2vec vector columns
        documents = [TaggedDocument(doc, [i]) for i, doc in
                     enumerate(appreviews["review_clean"].apply(lambda x: x.split(" ")))]
        # train a Doc2Vec model with our text data
        model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
        # transform each document into a vector data
        doc2vec_df = appreviews["review_clean"].apply(lambda x: model.infer_vector(x.split(" "))).apply(pd.Series)
        doc2vec_df.columns = ["doc2vec_vector_" + str(x) for x in doc2vec_df.columns]
        appreviews = pd.concat([appreviews, doc2vec_df], axis=1)

        tfidf = TfidfVectorizer(min_df=10)
        tfidf_result = tfidf.fit_transform(appreviews["review_clean"]).toarray()
        tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names())
        tfidf_df.columns = ["word_" + str(x) for x in tfidf_df.columns]
        tfidf_df.index = appreviews.index
        appreviews = pd.concat([appreviews, tfidf_df], axis=1)

        final_df = appreviews[appreviews["nb_words"] >= 1].sort_values("pos", ascending=False)[
            ["ID", "Text","pos","neg", "Star"]]
        positive_df = final_df[(final_df.pos > 0.45) & (final_df.Star < 3) & (final_df.neg < 0.3)]
        positive_df = positive_df[["ID", "Text", "Star"]]
        st.dataframe(positive_df)